\documentclass[a4paper,12pt]{article}

\usepackage[dutch]{babel}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[pdftex,bookmarks=true]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{parskip}
\usepackage{float}
\usepackage{subcaption}
\usepackage{amsmath}

\title{Samenvatting Probleem Oplossend Denken II \\ \large TIN 2 - HoGent}
\author{Lorenz Verschingel}

\begin{document}
\maketitle
\LARGE \textsc{Kansrekening}\normalsize
\section{Gebeurtenissen en hun kansen}
\subsection{Inleiding}
De kansrekening houdt zich bezig met de studie van gebeurtenissen of toevalsveranderlijken.

\subsection{Universum of uitkomstenruimte}
Het universum of de utkomstenruimte van een experiment is de verzameling van alle mogelijke uitkomsten van dit experiment en wordt genoteerd met $\Omega$.

Het is van belang dat de uitkomstenruimte volledig is: elke mogelijke
uitkomst van een experiment moet tot $\Omega$ behoren.
Bovendien moet elke uitkomst van een experiment overeenkomen met juist één element van $\Omega$.

\subsection{Gebeurtenis}
Een gebeurtenis is een deelverzameling van de uitkomstenruimte.
Een enkelvoudige of elementaire gebeurtenis is een \textit{singleton}.

Een \textit{samengestelde} gebeurtenis heeft cardinaliteit groter dan 1.

Gebeurtenissen die geen gemeenschappelijke uitkomsten hebben noemt men \textit{disjunct}.
Disjuncte gebeurtenissen kunnen dus nooit samen voorkomen.

\subsection{Kansen en kansruimte}
We wensen nu aan elke gebeurtenis A een getal te koppelen dat uitdrukt hoe waarschijnlijk het is dat deze gebeurtenis voorkomt bij het uitvoeren van een experiment.
We noemen dit getal de \textit{kans} of \textit{waarschijnlijkheid} van A, en we noteren deze kans als $P(A)$.

Het toekennen van kansen aan gebeurtenissen dient aan de volgende drie regels te voldoen:

\begin{enumerate}
\item Kansen zijn steeds positief: voor elke gebeurtenis $A$ geldt dat $P(A) \geq 0$.
\item De uitkomstenruimte heeft kans 1: $P(\Omega)=1$.
\item Wanneer $A$ en $B$ disjuncte gebeurtenissen zij dan is: $P(A\cup B)=P(A)+P(B)$.

Dit noemt men de \textbf{somregel}.
\end{enumerate}

Wanneer de functie $P$ aan de bovenstaande eigenschappen voldoet dan noemt ment het drietal $(\Omega, P(\Omega), P)$ een \textit{kansruimte}.

Kansen voldoen aan de volgende eigenschappen:
\begin{enumerate}
\item Voor elke gebeurtenis A geldt dat $P(\overline{A}) = 1-P(A)$

$1 = P(\Omega) = P(A\cup \overline{A}) = P(A) + P(\overline{A})$
\item De onmogelijke gebeurtenis heeft een kans nul: $P(\emptyset)=0$

$P(\emptyset)= P(\overline{\Omega}) = 1 - P(\Omega) = 0$
\item Als $A \subseteq B$ dan is $P(A)\leq P(B)$, dan geldt= $P(A) = P(B) - P(B\setminus A)$.
\item De \textbf{uitgebreide somregel} is: $P(A \cup B) = P(A) + P(B) - P(A \cap B)$

$P(A\cup B) = P(A \cup (B\setminus A))\\
= P(A) + P(B\setminus A)\\
= P(A) + P(B\setminus (A \cap B))\\
= P(A) +- P(B) - P(A \cap B)$
\end{enumerate}
\subsubsection{Eindig universum}
\textbf{Formule van Laplace}: $P(A)=\frac{\#(A)}{\#(\Omega)}$

De formule van Laplace is enkel van toepassing als alle kansen even waarschijnlijk zijn.

\subsection{Voorwaardelijke kansen en (on)afhankelijkheid van gebeurtenissen}
Als $P(B)>0$, dab is de \textbf{voorwaardelijke kans} dat $A$ voorkomt als gegeven is dat B voorkomt gedefinieerd is als:
$P(A|B)=\frac{P(A\cap B)}{P(B)}$

$P(A|B)$ spreken we uit als "De kans op A gegeven B".

Twee gebeurtenissen $A$ en $B$ zijn \textbf{onafhankelijk} als:
$P(A\cap B) = P(A)P(B)$

Bewijs: $P(A|B) = \frac{P(A\cap B)}{P(B)} =P(A) 
\Leftrightarrow P(A\cap B) = P(A)P(B)$

\subsubsection{De regel van Bayes}
$P(B_j|A) = 
\frac{P(B_j)P(P(A|B_j)}{\sum^n_{i=1}P(B_i)P(A|B_i)}$

Dit resultaat is belangrijk voor het opstellen van zogenaamde boomdiagrammen of beslissingsbomen in het kader van opeenvolgende beslissingen.

\section{Kans- of toevalsvariabelen}
\subsection{Inleiding}
Een \textit{kansvariabele} $X$ is een afbeelding van $\Omega$ naar $R$.
Deze afbeelding associeert met elke mogelijke uitkomst van een kansexperiment dus een reëel getal.

\subsection{Discrete kansvariabelen}
Een kansvariabele $X$ is \textit{discreet} wanneer $X$ slechts een eindig of aftelbaar oneindig aantal waarden aanneemt.
Dit wil zeggen dat: $bld(X)=\{x_1,x_2,\dots\}$.

$P(X=x_i)=P(\{\omega \cap \Omega |X(\omega)=x_i\})=f_X(x_i)$

\subsection{Continue kansvariabelen}
Een toevalsveranderlijke $X$ is \textit{continu} als er een functie $f_X$ van $R$ naar $R^+$ bestaat zodanig dat:
$F_X(x)=\int^x_{-\infty}f_X(y)dy$

De functie $f_X$ wordt de \textit{kansdichtheid} genoemd.

\subsection{Verwachtingswaarde en variantie}
\subsubsection{Discrete kansvariabele}
$E[x]= \sum_i^nx_iP(X=x_i)$

$E[x] = \mu$ = expected value = verwachtingswaarde = gemiddelde

$Var[x]=\sum_i^n(x_i-E[x])^2P(X=x_i)$

$Var[x] = \sigma^2$ = variantie = gemiddelde kwadratische afwijking

$\sigma$ = standaardafwijking

\subsubsection{Continue kansvariabele}
$E[x]=\int^\infty_{-\infty}xf_X(x)dx$

$Var[x]=\int^\infty_{-\infty}(x-\sigma_x)^2f_X(x)dx$

\subsubsection{Eigenschappen van verwachtingswaarde en variantie}
\begin{enumerate}
\item Als $X$ constant is, i.e. $X(\omega)=k$ voor alle elementen $\omega$ van  $\Omega$, dan is $E(X)=k$.
\item Als $a \in R$ een constante is, dan geldt:
$E(X+a)=E(X)+a)$

waaruit volgt dat: $E(X-\mu_X)=0$
\item Als $ a \in R$ een constante is, dan geldt:
$E(aX)=aE(X)$
\item Er geldt steeds dat:
$\sigma^2_X=E(X^2)-\mu^2_X$

Deze formule geeft de mogelijkheid om de variantie efficiënter te berekenen dan rechtstreeks via de definitie.
\item $\sigma^2_{X+a}=\sigma^2_X$
\item $\sigma^2_{aX}=a^2\sigma^2_X$
\end{enumerate}

\section{Kansverdelingen}
\subsection{Discrete kansverdelingen}
\subsubsection{De Bernoulliverdeling}
$x$ kan maar twee waarden aannemen.

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
x&0&1\\
\hline
$f_X(x)$ & 1-p & p\\
\hline
\end{tabular}
\end{table}
$\mu_X=0\times(1-p)+1\times p = p$

$\sigma^2_X=E[x^2]-E[x]^2=0^2\times (1-p)+1^2\times p - p^2 = p(1-p)=pq$

Het Bernouilliverdeling kan het best beschreven worden met het volgende voorbeeld:
één maal een muntstuk opgooien en kijken of je munt hebt.

\subsubsection{De binomiale verdeling}
$X\approx B(n,p)$

De binimiale verdeling kan het best beschreven worden met het volgende voorbeeld:
tien maal met een munt gooien en kijken hoeveel keer je munt hebt.

$f_X(k)=P(X=k)=C^k_np^kq^{n-k}=\binom{n}{k}p^kq^{n-k}$

met $\binom{n}{k}=\frac{n!}{k!(n-k)!}$

$\mu_X=np$

$\sigma^2_X=np(1-p)$

\subsubsection{De geometrische verdeling}
De geometrische verdeling kan het best beschreven worden met het volgende voorbeeld:
blijven gooien met een muntstuk tot je munt hebt.

$P(X=k) = (1-p)^{k-1}p$

$\mu_X = \frac{1}{1-q}$

$\sigma^2_X=\frac{q}{p^2}$

\textbf{De markov-eigenschap}

\begin{equation}
\begin{array}{rcl}
P(X>m+n|X>m)&=&\frac{P((X>m+n)\cap (X>n))}{P(X>n)}\\
&=& \frac{P(X>m+n)}{P(X>m)}\\
&=&\frac{q^{m+n}}{q^m}\\
&=&q^n\\
&=&P(X>n)
\end{array}
\end{equation}

\subsubsection{De Poisson verdeling}
De Poisson verdeling kan gezien worden als een binomiale verdeling met n  zeer groot en p $\approx$ 0.

$\Rightarrow np=\lambda$

$P(X=k)=e^{-\lambda}\frac{\lambda^k}{k!}$

$\mu_X = np=\lambda$

$\sigma^2_X=(np(1-p))=\lambda\times 1=\lambda$

\textbf{Het Poisson-proces}

$P(\text{k voorkomens in [0,t]})=e^{-\lambda t}\frac{(\lambda t)^k}{k!}$

\subsection{Continue kansverdelingen}
\subsubsection{Uniforme kansverdeling}
$\mu_X=\frac{a+b}{2}$

$\sigma^2_X=\frac{(b-a)^2}{12}$

\subsubsection{De exponentiële verdeling}
X $\rightarrow$ Poisson-proces met parameter $\lambda$.

T = tijd tot eerste voorkomen.

$P(T>t)=P(\text{0 voorkomens in[0,t]}) = e^{-\lambda t}\frac{(\lambda t)^k}{k!}=e^{-\lambda t}$

$\Rightarrow P(T\leq t) = 1-e^{-\lambda t}\text{ als t} \geq 0$

$\mu_T = \frac{1}{\lambda}$

$\sigma^2_T = \frac{1}{\lambda^2}$

De exponentiële verdeling voldoet aan de Markov-eigenschap: ze bezit geen geheugen.
\end{document}